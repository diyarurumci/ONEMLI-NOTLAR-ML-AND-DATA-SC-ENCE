 - UZAKLIK BAZLI HESAPLAMALARI KULLANAN ALGORİTMALAR İÇİN SCALE İŞLEMİ ÇOK ÖNEMLİDİR. SCALE EDİLMEYEN DURUMLARDA DEĞERLER SAPACAKTIR.
 BUNUN ÖNÜNE GEÇMEK İÇİN STANDARTİZASYON VE NORMALİZASYON GİBİ METODLAR KULLANILIR.
 MinMax Scaling, verinin 0 ile 1 arasında değerler aldığı bir durumdur. Burada dağılım, verinin dağılımı ile benzerdir. 
   Burada ‘outlier’ denilen dışta kalan verilere karşı hassasiyet durumu vardır, bu yüzden bu değerlerin fazla olduğu bir durumda iyi bir performans gösteremeyebilir.
 Robust Scaler, Normalizasyon ile benzer şekilde çalışır. Aykırı değerlere sahip verilerde daha iyi sonuçlar verebilir.Yine veri dağılımı ile benzerlik 
  gösterir ancak aykırı değerler dışarıda kalır. Medyan değeri sonradan kullanılmak üzere elenir ve değerler 1.ve 3. kartil aralığına oturtulur.
 MaxAbs Scaler, her özelliğin maksimum mutlak değeri 1 olacak şekilde her özelliği ayrı ayrı ölçeklendirir ve dönüştürülür.
- sola yatık (left skewness )  , sağa yatık datayla işlem yapmak yerine çeşitli araçlarla bunları ortalamak modelimie daha iyi bir veri sağlayacaktır
--Why we use fit_transform() on training data but transform() on the test data? 
  -After working on the previous model, we try to perform ensembling by combinig different models to predict the final outcome using StackingRegressor
- cros validasyonun önemini biliyoruz KFold ,stratified KFold (more balanced,targetin her yerinden örnek almaya çalışır), groupKFold , stratified gropKFold , time series split
- catboost is faster and better at analays at smal amount of data, pool parametrisne arraylaerimi sıralayabiliriz. modelimize tanıtmamızı sağlar.gpu mu cpu mu kararını 
ver. golden parametre
!! GPU VE CPU n_jobs  = kaç tane cpu corr kullanacığını belirtmek içindir. kaç tane kor varsa bilgisayarda ona göre seçebilirsin. -1 atarsan tümünü kullanılır kaç tan olduğuna
bakmana gerek kalmaz. njobsu model kurarken belirtmemize gerek yok cross validasyon işlemi sırasında eklersek daha iyi olabilir. CPU miktarını arttırmak işlemimizi daha hızlı 
yapmamızı sağlar.
-ok fazla eri veya feature varsa bunları modelin eğitmesi riskli olabilir bu durumda pca ile azaltabiliriz 
Veya çok boyutlu data ile görselleştirme yapmak ve bu görselleri anlamlı olarak incelemek imkansızdır. PCA, data görselleştirmesi amacıyla da kullanılmaktadır.
if we have a lot of featuers gradient boosting models will be bad we have to make them less or change the model
-categories of feature seleciton:
?filter based :1_correlation 2_variance threshold 3_chisquare 4_anova 5_information_gain
?wrapper based:1_rfe 2_
?embeded : 1_l1 l2 

-corelation and variance threshold: first make corelation matrix and check the high corelated features make a threshold and remowe one of them beacuse c'est pas necesasire
  - groupby, shifting ve transform  metodları ile lambda x fonksiyonunu da kullanarak yeni columnslar yaratabiliriz. aşağıda örnek kod veriyorum : 
  df["final roving avarage"] = df.gropby("group",as_index =False)["shift column"].transform(lambda x : x.rolling(3, min_periods = 1).mean())
  
  
